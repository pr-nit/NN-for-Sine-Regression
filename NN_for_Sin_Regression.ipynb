{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-for-Sin-Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hQboDaFwapvY",
        "MoUx1ySUTGgf",
        "-LXJ7plrc5HD"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQboDaFwapvY"
      },
      "source": [
        "# パターン認識・演習　ニューラルネットワークの基礎\n",
        "下記の点を目的とするニューラルネットワークの演習です。Moodleの解説動画を視聴したあと、上から順に説明を読み、プログラムを実行してください。目的は次の通りです。\n",
        "\n",
        "* ニューラルネットワークで回帰問題を解くことを体験する\n",
        "* 回帰において個々のニューロンの出力を基底関数と解釈出来ることを理解する\n",
        "* 初期値に依存して学習結果や結果に至る過程が異なりうることを理解する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAfBFYTrg0hz"
      },
      "source": [
        "# 0 各種モジュールの読み込み\n",
        "下記左の三角形をクリックして実行してください。プログラムの変更は不要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUpOr_QcerCj"
      },
      "source": [
        "# Macの問題回避\n",
        "# import os\n",
        "# import platform\n",
        "# if platform.system() == 'Darwin':\n",
        "#     os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Dense, Activation, Input\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoUx1ySUTGgf"
      },
      "source": [
        "# 1. 準備：活性化関数と線形変換\n",
        "この演習で作るNeural Networkでは中間層の活性化関数にtanhを採用する。準備として、tanh(x)に慣れておく"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8--sZJq4TxR7"
      },
      "source": [
        "# 1.1 y = tanh(x)の描画\n",
        "sigmoid関数と似ている形。値域が(-1,+1)であることに注意。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JeL31r1TlnV"
      },
      "source": [
        "# 関数を表示する定義域 [-10,10]\n",
        "x = np.linspace(-3.0,3.0,100 )\n",
        "y = np.tanh( x )\n",
        "# 描画\n",
        "plt.plot( x, y )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXJ7plrc5HD"
      },
      "source": [
        "# 1.2 y = tanh(w*x+b)の描画\n",
        "中間層のニューロンがどのような「基底関数」を獲得しうるかを確認します。\n",
        "結線の重みは、f(x) = tanh(wx+b)と表したときのwとbです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYfOeUoylUYa"
      },
      "source": [
        "# 1.2.1 wが負になると左右反転します\n",
        "下記を実行してwが負になると左右反転する。w=-1.0のときのグラフを確認すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLMamuulaqZ"
      },
      "source": [
        "# 係数の符号を反転させて描画する\n",
        "for w in (-1.0, 1.0):\n",
        "  y = np.tanh( w*x )\n",
        "  plt.plot( x, y, label=f'w={w}')\n",
        "plt.legend(bbox_to_anchor=(1, 1) )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UUOseOZk7NP"
      },
      "source": [
        "# 1.2.2 |w|が変化すると、入力される変数xの縮尺が変わります。\n",
        "|w|を1より小さくすると「横に広がる」ことになり、1より大きくすると「横に縮む」。描画されるグラフそれぞれのwの値を確認すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EX70ZgRXK28"
      },
      "source": [
        "# 係数wを2^iで表して、i=-2, -1, 0, 1, 2で変化させる\n",
        "for i in range(-2, 3):\n",
        "  w = 2.0**i\n",
        "  y = np.tanh( w*x )\n",
        "  plt.plot( x, y, label=f'w={w}' )\n",
        "plt.legend(bbox_to_anchor=(1, 0), loc='lower right' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Y7Y-gGm76o"
      },
      "source": [
        "# 1.2.3 bが変化すると左右に並進します\n",
        "bが変化するとグラフの位置が変化する。描画されるグラフそれぞれのbの値を確認すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj3Dy42OU4A-"
      },
      "source": [
        "# w=1のままbを変化させる\n",
        "w=1.0\n",
        "for b in range(-3, 4):\n",
        "  y = np.tanh( w*x + b )\n",
        "  plt.plot( x, y, label=f'b={b}')\n",
        "plt.legend(bbox_to_anchor=(1,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihLsc5mhUoF"
      },
      "source": [
        "# 2. 実験の開始：訓練データの生成\n",
        "訓練用のデータ (x,y)と訓練中の性能評価用のデータ(x_test, y_test)を生成します。訓練用のデータも評価用データもNUM個ずつ生成します。\n",
        "* *x*: 0以上1未満の乱数。 サイズ NUM の配列\n",
        "* *y*: *y* = sin (2π*fx* )\n",
        "\n",
        "下記プログラム中の変数は次のとおり\n",
        "* pi: 円周率\n",
        "* NUM: 訓練データの数・評価用データの数\n",
        "* FREQ: 回帰したい関数は三角関数。その周波数。\n",
        "\n",
        "下記二つのセルを、三角形をクリックすることで上から順に実行してください。\n",
        "2番目の三角形をクリックすると、生成したデータのグラフが表示されます。FREQ=2の場合、2つの波が続くグラフになるはずです。（点が密集して分かりにくいですが、青い点は順序のランダムなNUM個の点です）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUk66OMgerCn"
      },
      "source": [
        "# 訓練データの生成\n",
        "# 円周率　π\n",
        "pi = np.pi\n",
        "# 周波数\n",
        "FREQ = 2.0\n",
        "# 訓練データの数・評価用\n",
        "NUM = 500\n",
        "# 誤差の標準偏差\n",
        "SCALE = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKvAzZ9NerCn"
      },
      "source": [
        "# 学習データの生成 (eは誤差)\n",
        "e = np.random.normal(loc=0.0, scale=SCALE, size=NUM)\n",
        "x = np.random.rand( NUM )\n",
        "y = np.sin( 2.0*pi*FREQ*x ) + e\n",
        "# 評価データの生成\n",
        "e_test = np.random.normal(loc=0.0, scale=SCALE, size=NUM)\n",
        "x_test = np.random.rand( NUM )\n",
        "y_test = np.sin( 2.0*pi*FREQ*x_test ) + e_test\n",
        "# 学習データのグラフの表示（横軸x, 縦軸y)\n",
        "plt.scatter(x,y)\n",
        "plt.title('Training Data')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxmnLmuAyRh_"
      },
      "source": [
        "# 3. ニューラルネットワークの構築\n",
        "次に、入力→中間層→出力 の3層からなるニューラルネットワークをデザインします。入力xも出力yもスカラなので、入力層のニューロンは1つ、出力層のニューロンの出力も1つです。\n",
        "\n",
        "プログラムの読み方を簡単に解説します。\n",
        "\n",
        "* **model = Sequential():** 層を積み重ねたニューラルネットワークをこれから作る宣言です。\n",
        "* **model.add:**  層を追加します。2層目から順に追加します\n",
        "  * 1層目は2層目を作るときにデザイン出来てしまいます\n",
        "* **Dense:** 全結合の層を追加します。つまり、前の層の全てのニューロンを、いま作る層のニューロン全てと結合させます。\n",
        "  * 引数によりニューロンの数と活性化関数と入力の次元を指定します。\n",
        "  * 2層目はZ_NUM個のニューロンを用意して、それらの活性化関数はtanhにして、前の層の出力は1次元とします。\n",
        "  * 3層目はニューロンは一つで、活性化関数は恒等関数(linear)です。\n",
        "* **optimizers:** 最適化の準備をします。ここではStatistical Gradient Descentを採用します。\n",
        "  * **learning_rate:** 勾配ベクトルに掛ける係数（資料ではε）\n",
        "  * **decay:**Learning rateを学習が進むにつれて小さくするための係数です。学習の初期には大胆に係数を更新して、次第に更新に慎重になるための仕掛けです。\n",
        "* **model.compile:**コスト関数と最適化法を指定します\n",
        "  * **loss='mean_square_error':** 二乗誤差をコストにします。\n",
        "  * **'sgd':** 確率的勾配法で最適化します。\n",
        "\n",
        "下のセルを三角形を上から順にクリックすることで実行してください。最初のセルで、2層目のニューロンの数を指定するための変数Z_NUMを定めます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el_7r_QG4_he"
      },
      "source": [
        "# 中間層のニューロンの数\n",
        "Z_NUM = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG8AKK-RerCo"
      },
      "source": [
        "# 層の構築\n",
        "model = Sequential()\n",
        "# 2層目の追加\n",
        "model.add(Dense(Z_NUM, activation = 'tanh', input_dim = 1))\n",
        "# 3層目の追加\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "# 確率的勾配法のデザイン\n",
        "sgd = optimizers.SGD( learning_rate = 0.01, decay = 1e-6 )\n",
        "# コスト関数(loss関数)と最適化法の指定\n",
        "model.compile( loss = 'mean_squared_error', optimizer = 'sgd' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gszX43dm3klv"
      },
      "source": [
        "# 4. 学習\n",
        "学習を開始します。\n",
        "* **model.fit:** 最適化計算により係数を更新します。\n",
        "  * **epochs:** ニューラルネットワークの重み係数を更新する回数を指定します。全てのデータを1回ずつ使って更新すると1 epochです。\n",
        "  * **batch_size:** 沢山ある学習データのうち、batch_size個のデータのみをランダムに選んで更新に使います。1 epochの更新のためにはバッチの選択と更新を複数回実行しなければいけません。\n",
        "* **model.evaluate:** 学習には使っていないデータにより現状のニューラルネットワークの性能を評価します。\n",
        "* **print(score):** 1pochごとに評価値を印字します。\n",
        "\n",
        "下のセルを、三角形を上から順にクリックすることで実行してください。最初のセルで、パラメータの更新回数を指定するための変数EPOCHを定めます。\n",
        "\n",
        "EPOCH=20000のとき、実行時間は20分程度かかります。気長に待って下さい。ニューラルネットワークの係数の初期値が毎回違うので、学習過程は実行のたびに異なります。lossの値が0.1より小さくなれば「成功」です。lossの下がり方は一様ではなく、途中で急激に下がりだすこともあります。\n",
        "\n",
        "運悪くlossが下がらないこともあります。何度か実行して下さい。**再実行するときは「3. ニューラルネットワークの構築」に遡って、順に実行し直して下さい。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXQT3L706VHP"
      },
      "source": [
        "# パラメータの更新回数の指定\n",
        "EPOCH = 20000\n",
        "# バッチサイズ（一度に見るデータの数）\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQJaHGFerCp"
      },
      "source": [
        "###### 学習開始\n",
        "result = model.fit(x, y, epochs=EPOCH, batch_size = BATCH_SIZE)\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWAVxciBv3M"
      },
      "source": [
        "# 5. 学習過程の確認\n",
        "横軸にエポック数、縦軸にコスト関数の値を示すグラフを表示します。多くの人は、ロスの変化が一様ではないことに気付くことになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewnRPc8yerCp"
      },
      "source": [
        "# Lossの変化の可視化（横軸：エポック、縦軸：コスト関数）\n",
        "plt.plot(range(1, EPOCH+1), result.history['loss'], label=\"LOSS\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('LOSS')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fOEYftRCTi5"
      },
      "source": [
        "# 6. 学習結果の確認\n",
        "学習済みのニューラルネットワークに新しいデータを入力して、出力を確認する\n",
        "* **test_input:** 新規に生成したデータ。0から1まで0.01刻みで値を生成\n",
        "* **model_predict:**学習済みニューラルネットワークによる出力 \n",
        "\n",
        "表示されるグラフでは、細い赤線がニューラルネットワークによる回帰結果を示す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5L0dTkQEnH9"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "###### 学習データの描画\n",
        "plt.scatter(x,y, label='Training Data')\n",
        "\n",
        "###### 新規入力データに対する予測結果の描画\n",
        "test_input = np.arange(0,1,0.01)\n",
        "test_output = model.predict(test_input)\n",
        "test_output.reshape(len(test_output),)\n",
        "plt.plot(test_input, test_output, label='Regression', color='red', linewidth=5.0)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENdeWd2c84V"
      },
      "source": [
        "# 7. 各ニューロンが獲得した基底関数の表示\n",
        "下記を実行すると、Z_NUM個のニューロンが獲得した基底関数を色を分けて表示されます。位置やスケールの異なるtanh関数を足し合わせることにより、回帰を実現したということです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgMJ7uh9erCq"
      },
      "source": [
        "###### Visualization\n",
        "# Get Coefficietns\n",
        "a = model.layers[0].get_weights()\n",
        "b = model.layers[1].get_weights()\n",
        "\n",
        "# 関数を表示する定義域\n",
        "u = (np.arange(0,1,0.01))[:,np.newaxis] # column vector\n",
        "# 中間層への入力・重みとの線形演算\n",
        "w = np.dot(u, a[0])+a[1]\n",
        "# 活性化関数 (tanh)\n",
        "v = np.tanh(w)\n",
        "# 各ニューロンの出力の計算\n",
        "out = np.dot(v,b[0])+b[1]\n",
        "# 表示\n",
        "plt.scatter(x,y, label='Training data')\n",
        "plt.plot(u,out, label='Regression', color='red', linewidth=5.0)\n",
        "for i in range(Z_NUM):\n",
        "    plt.plot(u,v[:,i], label=i)\n",
        "plt.legend(bbox_to_anchor=(1.05, 0.5, 0.5, .100), \n",
        "           borderaxespad=0.,\n",
        "           ncol=1,\n",
        "           mode=\"expand\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYBGOg7Jx9XQ"
      },
      "source": [
        "# 8. ニューラルネットワークの利用上の注意（外挿について）\n",
        "データの「外挿」については注意が必要です。このことを確認します。\n",
        "\n",
        "**学習データに含まれていなかった領域（今回の例でいえば、横軸に沿って学習データが分布している範囲（0〜1）の外側）について回帰関数がどのような形になるかは、学習時には何も考えていない。**　特に学習データが少ないときには、このことを忘れがちなので注意してください。異常検知に利用できるかどうかにも関わる問題です。\n",
        "\n",
        "下記を実行して、（当然のことながら）データの与えられている部分だけで回帰がなされていることを確認してください。赤線が回帰の結果です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRagrF3bxQvH"
      },
      "source": [
        "###### 学習データの描画\n",
        "plt.scatter(x,y, label='Training Data')\n",
        "\n",
        "###### 新規入力データに対する予測結果の描画（今回は外挿する）\n",
        "test_input = np.arange(-2,3,0.01)\n",
        "test_output = model.predict(test_input)\n",
        "test_output.reshape(len(test_output),)\n",
        "plt.plot(test_input, test_output, label='Regression', color='red', linewidth=2.0)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aJg0K2fB1Yh"
      },
      "source": [
        "# 課題: 下記それぞれの画像を保存し、Moodleの当該箇所に提出せよ。\n",
        "\n",
        "1.   最初の一通りの実行結果のうち、次の画像を保存して当該箇所に提出せよ。\n",
        "* (1) 5.「学習過程の確認」の実行結果のグラフ\n",
        "* (2) 7.「各ニューロンが獲得した基底関数の表示」の実行結果のグラフ\n",
        "* (3) 8.「ニューラルネットワークの利用上の注意」の実行結果のグラフ\n",
        "2.   プログラムを修正せずに3.「ニューラルネットワークの構築」から順に最後まで再度実行せよ。ニューラルネットワークの重みの初期値が違うため、全く同じプログラムであるにも関わらず最初とは（微妙に）異なる実行結果が得られる。特に基底関数の組み合わせは大きく変化する場合が多い。\n",
        "* (1) 7.「各ニューロンが獲得した基底関数の表示」の実行結果のグラフ\n",
        "* (2) 8.「ニューラルネットワークの利用上の注意」の実行結果のグラフ\n",
        "3.   3.「ニューラルネットワークの構築の最初のセルをZ_NUM=6からZ_NUM=3に修正して、当該セルから順に最後まで再度実行せよ。基底関数の数がサイン関数を回帰するには不足するため回帰に失敗する。\n",
        "* (1) 5.「学習過程の確認」の実行結果のグラフ\n",
        "* (2) 7.「各ニューロンが獲得した基底関数の表示」の実行結果のグラフ\n",
        "* (3) 8.「ニューラルネットワークの利用上の注意」の実行結果のグラフ\n",
        "\n"
      ]
    }
  ]
}